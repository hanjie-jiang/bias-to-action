---
title: ML Fundamentals Overview
---

# Machine Learning Fundamentals

Welcome to the Machine Learning Fundamentals section! This comprehensive guide covers the essential concepts and techniques that form the foundation of machine learning.

## ðŸ“š Learning Path

### 1. **Feature Engineering**
The foundation of any successful ML project starts with proper data preparation and feature engineering.

- **[Data Types & Normalization](feature_engineering/data_types_and_normalization.md)** - Understanding structured vs unstructured data, normalization techniques (min-max scaling, z-score)
- **[Categorical Encoding](feature_engineering/categorical_encoding.md)** - Ordinal, one-hot, and binary encoding methods
- **[Feature Crosses](feature_engineering/feature_crosses.md)** - High-dimensional feature combinations and dimensionality reduction

### 2. **Model Evaluation & Validation**
Learn how to properly evaluate and validate your machine learning models.

- **[Evaluation Methods](model_evaluation/evaluation_methods.md)** - Holdout, cross-validation, and bootstrap methods
- **[Metrics & Validation](model_evaluation/metrics_and_validation.md)** - Accuracy, precision, recall, F1-score, and ROC curves
- **[Hyperparameter Tuning](model_evaluation/hyperparameter_tuning.md)** - Grid search, random search, and Bayesian optimization

### 3. **Regularization & Overfitting**
Master techniques to prevent overfitting and improve model generalization.

- **[Overfitting & Underfitting](regularization/overfitting_underfitting.md)** - Detection and mitigation strategies
- **[L1/L2 Regularization](regularization/l1_l2_regularization.md)** - Mathematical foundations and implementation
- **[Early Stopping](regularization/early_stopping.md)** - Training control and data augmentation techniques

### 4. **Classical Supervised Algorithms**
Explore fundamental supervised learning algorithms.

- **[Linear Regression](classical_algorithms/linear_regression.md)** - Simple and multiple linear regression with implementation
- **[Logistic Regression](classical_algorithms/logistic_regression.md)** - Binary classification with sigmoid function and maximum likelihood
- **[Decision Trees](classical_algorithms/decision_trees.md)** - Gini impurity, information gain, and tree construction

### 5. **Unsupervised Learning**
Discover algorithms for finding patterns in unlabeled data.

- **[K-Nearest Neighbors](unsupervised_learning/k_nearest_neighbors.md)** - Distance metrics and classification algorithm
- **[K-Means Clustering](unsupervised_learning/k_means_clustering.md)** - Clustering algorithm and implementation

## ðŸ”— Related Topics

- **[Probability & Markov](../probability_and_markov/Probability_and_Markov_Overview.md)** - Probability foundations and Bayesian methods
- **[Language Models](../language_model/Ngram_Language_Modeling.md)** - Text processing and NLP techniques
- **[Information Theory](../Information_Theory.md)** - Entropy, cross-entropy, and KL divergence
- **[Linear Algebra](../Linear_Algebra_for_ML.md)** - Mathematical foundations for ML
- **[Calculus & Gradient Descent](../Calculus_and_Gradient_Descent.md)** - Optimization techniques

## ðŸŽ¯ Key Learning Objectives

By the end of this section, you will understand:

- How to preprocess and engineer features for ML models
- Methods for evaluating and validating model performance
- Techniques to prevent overfitting and improve generalization
- Implementation of fundamental supervised and unsupervised algorithms
- Best practices for hyperparameter tuning and model selection

## ðŸ“– Prerequisites

- Basic understanding of Python programming
- Familiarity with probability concepts (see Probability & Markov section)
- Knowledge of linear algebra fundamentals (see Linear Algebra for ML)

---

*Start with Feature Engineering to build a solid foundation, then progress through the sections in order for the best learning experience.*
