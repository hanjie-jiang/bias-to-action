
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://hanjie-jiang.github.io/ml-learning-notes/ml_fundamentals/ML_fundamentals/">
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.17">
    
    
      
        <title>ML Fundamentals - Machine Learning Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/styles/layout.css">
    
      <link rel="stylesheet" href="../../assets/styles/hero.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#feature-engineering" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Machine Learning Notes" class="md-header__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ML Fundamentals
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Machine Learning Notes" class="md-nav__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Machine Learning Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Engineering and Data Structure
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Engineering and Data Structure
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Overview/Engineering_and_Data_Structure_Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Data Structures
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Data Structures
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_2_1" id="__nav_2_2_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Arrays
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2_1">
            <span class="md-nav__icon md-icon"></span>
            Arrays
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Arrays/Arrays_Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Arrays Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Arrays/Dynamic_Arrays/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dynamic Arrays
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Arrays/Array_Problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Array Problems
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2_2" id="__nav_2_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Hash Tables
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2_2">
            <span class="md-nav__icon md-icon"></span>
            Hash Tables
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Hash_Tables/Hash_Tables_Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hash Tables Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Hash_Tables/Hash_Functions_and_Collisions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hash Functions & Collisions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Hash_Tables/Python_Dictionaries/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python Dictionaries
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Hash_Tables/Python_Dictionary_Operations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python Dictionary Operations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Hash_Tables/Python_Sets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python Sets
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Hash_Tables/Python_Set_Operations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python Set Operations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Hash_Tables/String_Operations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    String Operations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Hash_Tables/Hash_Table_Problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hash Table Problems
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_2_3" id="__nav_2_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Recursion
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2_3">
            <span class="md-nav__icon md-icon"></span>
            Recursion
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Recursion/Recursion_Fundamentals/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recursion Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Recursion/Recursive_Algorithms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recursive Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Recursion/Recursion_vs_Iteration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recursion vs Iteration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Data_Structures/Recursion/Common_Recursive_Patterns/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Common Recursive Patterns
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Algorithms
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Algorithms
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3_1" >
        
          
          <label class="md-nav__link" for="__nav_2_3_1" id="__nav_2_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Search Algorithms
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3_1">
            <span class="md-nav__icon md-icon"></span>
            Search Algorithms
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithms/Search_Algorithms/Search_Algorithms_Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Search Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithms/Search_Algorithms/Binary_Search_Fundamentals/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Binary Search
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithms/Search_Algorithms/Binary_Search_Variations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Binary Search Variations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithms/Search_Algorithms/Search_Problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Search Problems
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3_2" >
        
          
          <label class="md-nav__link" for="__nav_2_3_2" id="__nav_2_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Sorting Algorithms
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3_2">
            <span class="md-nav__icon md-icon"></span>
            Sorting Algorithms
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithms/Sorting_Algorithms/Sorting_Algorithms_Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sorting Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithms/Sorting_Algorithms/Sorting_Problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sorting Problems
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Algorithmic Patterns
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            Algorithmic Patterns
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4_1" >
        
          
          <label class="md-nav__link" for="__nav_2_4_1" id="__nav_2_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Two Pointers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4_1">
            <span class="md-nav__icon md-icon"></span>
            Two Pointers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithmic_Patterns/Two_Pointers/Two_Pointers_Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Two Pointers Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithmic_Patterns/Two_Pointers/Opposite_Direction_Pointers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Opposite Direction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithmic_Patterns/Two_Pointers/Same_Direction_Pointers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Same Direction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithmic_Patterns/Two_Pointers/Two_Pointers_Problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Two Pointers Problems
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4_2" >
        
          
          <label class="md-nav__link" for="__nav_2_4_2" id="__nav_2_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Sliding Window
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4_2">
            <span class="md-nav__icon md-icon"></span>
            Sliding Window
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithmic_Patterns/Sliding_Window/Sliding_Window_Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sliding Window Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithmic_Patterns/Sliding_Window/Fixed_Size_Window/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fixed Size Window
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithmic_Patterns/Sliding_Window/Variable_Size_Window/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Variable Size Window
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Algorithmic_Patterns/Sliding_Window/Sliding_Window_Problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sliding Window Problems
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5" >
        
          
          <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Problem Solving
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_5">
            <span class="md-nav__icon md-icon"></span>
            Problem Solving
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5_1" >
        
          
          <label class="md-nav__link" for="__nav_2_5_1" id="__nav_2_5_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Set & Dictionary Problems
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_5_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_5_1">
            <span class="md-nav__icon md-icon"></span>
            Set & Dictionary Problems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Problem_Solving/Set_Dictionary_Problems/Array_Intersection/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Array Intersection
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Problem_Solving/Set_Dictionary_Problems/Non_Repeating_Elements/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Non-Repeating Elements
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Problem_Solving/Set_Dictionary_Problems/Unique_Elements/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unique Elements
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Problem_Solving/Set_Dictionary_Problems/Anagram_Pairs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Anagram Pairs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5_2" >
        
          
          <label class="md-nav__link" for="__nav_2_5_2" id="__nav_2_5_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    String Problems
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_5_2">
            <span class="md-nav__icon md-icon"></span>
            String Problems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Problem_Solving/String_Problems/Unique_Strings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unique Strings
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_6" >
        
          
          <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Resources
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_6">
            <span class="md-nav__icon md-icon"></span>
            Resources
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Resources/Common_Patterns/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Common Patterns
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Resources/Time_Complexity_Guide/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Time Complexity Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engineering_and_data_structure/Resources/Interview_Strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Interview Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    ML Foundations and Algorithms
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            ML Foundations and Algorithms
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Probability & Markov
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Probability & Markov
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability_and_markov/Probability_and_Markov_Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability_and_markov/probability_and_markov_sections/conditional_probability_and_bayes_rule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Bayesâ€™ Rule
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability_and_markov/probability_and_markov_sections/naive_bayes_and_gaussian_naive_bayes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Naive Bayes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability_and_markov/probability_and_markov_sections/joint_and_marginal_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Joint & Marginal
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Linear Algebra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../calculus_and_linear_algebra/Linear_Algebra_for_ML/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Algebra for ML
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../calculus_and_linear_algebra/Calculus_and_Gradient_Descent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Calculus and Gradient Descent
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    ML Fundamentals
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            ML Fundamentals
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ML_Fundamentals_Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_3_2" id="__nav_3_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Feature Engineering
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_2">
            <span class="md-nav__icon md-icon"></span>
            Feature Engineering
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../feature_engineering/data_types_and_normalization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Types & Normalization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../feature_engineering/categorical_encoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Categorical Encoding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../feature_engineering/feature_crosses/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Feature Crosses
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3_3" id="__nav_3_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Model Evaluation
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_3">
            <span class="md-nav__icon md-icon"></span>
            Model Evaluation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../model_evaluation/evaluation_methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluation Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../model_evaluation/metrics_and_validation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Metrics & Validation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../model_evaluation/hyperparameter_tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hyperparameter Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_3_4" id="__nav_3_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Regularization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_4">
            <span class="md-nav__icon md-icon"></span>
            Regularization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../regularization/overfitting_underfitting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overfitting & Underfitting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../regularization/l1_l2_regularization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    L1/L2 Regularization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../regularization/early_stopping/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Early Stopping
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_3_5" id="__nav_3_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Classical Algorithms
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_5">
            <span class="md-nav__icon md-icon"></span>
            Classical Algorithms
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../classical_algorithms/linear_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Regression
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../classical_algorithms/logistic_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logistic Regression
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../classical_algorithms/decision_trees/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Decision Trees
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3_6" >
        
          
          <label class="md-nav__link" for="__nav_3_3_6" id="__nav_3_3_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Unsupervised Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_6">
            <span class="md-nav__icon md-icon"></span>
            Unsupervised Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../unsupervised_learning/k_nearest_neighbors/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    K-Nearest Neighbors
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../unsupervised_learning/k_means_clustering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    K-Means Clustering
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../language_model/Ngram_Language_Modeling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Language Model
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Neural Networks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Neural Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../neural_networks_and_deep_learning/Neural_Networks_and_Deep_Learning_Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../neural_networks_and_deep_learning/neural_networks_sections/Introduction_to_Perceptron_Algorithm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Perceptron Algorithm
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>ML Fundamentals</h1>

<h2 id="feature-engineering">Feature Engineering<a class="headerlink" href="#feature-engineering" title="Permanent link">&para;</a></h2>
<p>Two types of data:
- Structured / Tabular data: Could be viewed as a data table from the relational database, which every columns has their clear definition, including <strong>numerical</strong> and <strong>categorial</strong> data types.
- Unstructured data: Includes <strong>text, image, audio, video data</strong>, and the information that this type of data contains cannot be represented easily as a numerical value, and also they do not have clear categorical definition, furthermore, the size of these data are not identical.</p>
<h3 id="normalization-of-features">Normalization of Features<a class="headerlink" href="#normalization-of-features" title="Permanent link">&para;</a></h3>
<h4 id="why-does-one-need-to-do-normalization-on-numerical-features">Why does one need to do normalization on numerical features?<a class="headerlink" href="#why-does-one-need-to-do-normalization-on-numerical-features" title="Permanent link">&para;</a></h4>
<p>In order to eliminate the magnitude impact between features, we should always do normalization to the features that we use, i.e. to uniformly normalize all the features to a similar range, so that it could <strong>help compare between different metrics</strong>. There are two different types of normalization that people most commonly use:
- <em>min-max scaling</em>: It linearly changes the original data so that the data could be projected to [0, 1] range so that it is an equal ratio transformation of the original data:
<span class="arithmatex">\(<span class="arithmatex">\(X_{\text{norm}} = \frac{X-X_{\text{min}}}{X_{\text{max}-X_{\text{min}}}}\)</span>\)</span>
- <em>Z-Score normalization</em>: It would project the original data to a mean of 0 and variance = 1 distribution. Specifically, assume that the original feature has mean <span class="arithmatex">\(\mu\)</span> and variance <span class="arithmatex">\(\sigma\)</span> , then the normalization equation would be defined as:
<span class="arithmatex">\(<span class="arithmatex">\(Z = \frac{x-\mu}{\sigma}\)</span>\)</span> 
Using stochastic gradient descent (SGD) as an example, when two numerical features, <span class="arithmatex">\(x_1\)</span> of range [0,10] and <span class="arithmatex">\(x_2\)</span> of range [0,3], then when the <span class="arithmatex">\(x_1\)</span> and <span class="arithmatex">\(x_2\)</span> are not normalized, the <img alt="Screenshot" src="../resources/Screenshot%202025-08-05%20at%208.22.11%20PM.png" /> gradient descent would not be as efficient as when one does the normalization of the features. However, feature normalization is not always working. In real life, <span style="background-color: #FEE9E7"> whenever a model utilizes SGD, it is suggested to use the normalization, including linear regression, logistic regression, support vector machine, neural networks, whereas decision tress it does not help. </span> As for decision tree models, the node split usually is determined by the data and how much <a href="https://en.wikipedia.org/wiki/Information_gain_ratio"><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>information gain ratio</a> that data contains about X. This information gain ratio is not impacted by whether the feature has been normalized, rather it would not change the information gain of the specific feature X.</p>
<h3 id="categorical-features">Categorical Features<a class="headerlink" href="#categorical-features" title="Permanent link">&para;</a></h3>
<p>Categorical features include male / female, blood type (A,B,AB,O) and etc, which can only select values from a finite set of choices. Categorical features original input are mostly strings. Despite that <strong>decision trees and some other numbers of models can directly take in the strings, for logistic regression or SVM models, the categorical features need to be translated to numerical form</strong> so that they could properly work.</p>
<h6 id="how-to-do-feature-engineering-on-categorical-features">How to do feature engineering on categorical features?<a class="headerlink" href="#how-to-do-feature-engineering-on-categorical-features" title="Permanent link">&para;</a></h6>
<p>One would need to encode the features to a higher dimensional vector to represent them in the model.
- <strong>ordinal encoding</strong>: usually used to treat those data that has ordinal sequence, for example when scoring we have high &gt; middle &gt; low, then the ordinal encoder would help to describe this type of sequence via giving it a numerical ID. For example, we could represent high as 3, middle as 2 and low as 1 in this case, which helps retain the high to low relationship.
- <strong>one-hot encoding</strong>: usually used to treat features that do not have ordinal relationships, for example, for blood type, one could directly use the [1,0,0,0], [0,1,0,0], [0,0,1,0] and [0,0,0,1] to represent the different types. Note:
    - use of sparse vector for saving space
    - high-dimensional features can be difficult in following scenarios: 1) K-nearest neighbors, the distance between two high-dimensional vectors can be hard to measure, 2) logistic regression, the parameters can increase with higher dimensions, thus causing overfitting problems and 3) only some of the dimensions could be helpful when doing clustering or predictions, so one could think to reduce dimensions with feature selections.
- <strong>binary encoding</strong>: using binary to do a hash mapping on the original category ID, this can help save space when comparing with the one-hot encoding as it is usually of fewer dimensions.</p>
<h3 id="high-dimensional-feature-crosses">High Dimensional Feature Crosses<a class="headerlink" href="#high-dimensional-feature-crosses" title="Permanent link">&para;</a></h3>
<h4 id="what-are-feature-crosses-and-how-to-deal-with-high-dimensional-feature-crosses">What are feature crosses? And how to deal with high-dimensional feature crosses?<a class="headerlink" href="#what-are-feature-crosses-and-how-to-deal-with-high-dimensional-feature-crosses" title="Permanent link">&para;</a></h4>
<p>Using single features to combine them together via dot-product or inner-product, one can get a combination of two features to help represent nonlinear relationships.</p>
<p>Using logistic regression as an example, when a data set contains feature vector <span class="arithmatex">\(X=(x_1, x_2, ..., x_k)\)</span> then one would have <span class="arithmatex">\(Y = \text{sigmoid}(\sum_i \sum_j w_{ij} \langle x_i, x_j \rangle)\)</span> . <span class="arithmatex">\(w_{ij}\)</span> is of dimension <span class="arithmatex">\(n_{x_i} \cdot n_{x_j}\)</span> . But when <span class="arithmatex">\(n_{x_i} \times n_{x_j}\)</span> is huge, especially in use cases of website customers and number of goods, this can be really huge dimension. So **one way to get around this is to use a k-dimensional low-dimension vector (k &lt;&lt; m, k &lt;&lt; n). Now,  <span class="arithmatex">\(w_{ij} = x_i' \cdot x_j'\)</span> and now the number of parameters one needs to tune is <span class="arithmatex">\(m \times k + n \times k\)</span> . This can also be viewed as the <a href="https://lumingdong.cn/recommendation-algorithm-based-on-matrix-decomposition.html"><sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>matrix vectorization</a>, that has been widely used in the recommendation systems. **</p>
<p>We have understood how to use dimension reduction to reduce the number of parameters that the model needs to learn given a feature cross of two high-dimensional features. <span style="background-color: #FEE9E7"> But in reality, we are facing a variety of high-dimensional features. So a single feature crosses of all the different pairs would induce 1) too many parameters and 2) overfitting issues. </span></p>
<h4 id="how-to-effectively-select-the-feature-combinations">How to effectively select the feature combinations?<a class="headerlink" href="#how-to-effectively-select-the-feature-combinations" title="Permanent link">&para;</a></h4>
<p>We introduce a feature cross selection based on decision tree models. Taking CTR prediction as an example, assume that the input includes age, gender, user type (free vs paid), searched item type (skincare vs foods), etc. We could thus make a decision tree from the original input and their labels. <img alt="Screenshot" src="../resources/Screenshot%202025-08-05%20at%209.30.27%20PM.png" /> We could then view the feature crosses from the tree, that contains four different type of pairs:
1. age + gender
2. age + searched item type
3. paid user + search item type
4. paid user + age
How to best construct the decision trees? One can use the <a href="https://medium.com/@ruchi.awasthi63/gradient-boosted-decision-tree-clearly-explained-bd1d8c7d9923">Gradient Boosting Decision TreeÃ¯Â¼Å’GBDT</a> or use<a href="https://neptune.ai/blog/gradient-boosted-decision-trees-guide"> the link</a> to get a better idea of the algorithm. The idea behind is that whenever before constructing a decision tree, we first calculate the error from the true value and iteratively construct the tree from the error.</p>
<h3 id="textual-descriptive-models">Textual Descriptive Models<a class="headerlink" href="#textual-descriptive-models" title="Permanent link">&para;</a></h3>
<p>Related Content: <a href="../../language_model/Ngram_Language_Modeling/">Ngram_Language_Modeling</a></p>
<p>Text is a category of unstructured data. How to work with textual data has always been one of the most important research directions.</p>
<h4 id="what-are-some-of-the-textual-descriptive-models-what-pros-and-cons-each-have">What are some of the textual descriptive models what pros and cons each have?<a class="headerlink" href="#what-are-some-of-the-textual-descriptive-models-what-pros-and-cons-each-have" title="Permanent link">&para;</a></h4>
<ul>
<li>Bag of words: Consider each article as a bag of words, ignoring the sequence of how each word appears. Specifically, it separates the entire paragraph of texts at word unit and represent each paragraph as a long vector. Each dimension in the vector is a word, and the weight represents how important the word is in the original article. </li>
<li>TF-IDF (Term Frequency-Inverse Document Frequency): Is often used to calculate the weight of the words, <span class="arithmatex">\(\text{TF-IDF}(t,d)=\text{TF}(t,d) \times \text{IDF}(t)\)</span> , where <span class="arithmatex">\(\text{TF}(t,d)\)</span> represents the frequency of word t in document d, whereas <span class="arithmatex">\(\text{IDF}(t)\)</span> is the reverse document frequency to measure word t's importance in grammar, corresponds to equation <span class="arithmatex">\(<span class="arithmatex">\(\text{IDF}(t) = log^{\frac{\text{total article}}{\text{total article that contains word} t +1}}\)</span>\)</span> the general meaning behind is that if a word appears so in various articles, then it means that it is a commonly used word, hence it would not contribute much in differentiating the specific meaning behind each articles, hence it should be penalized when weighting.</li>
<li>N-gram: when "natural language processing" being separated into 3 words as word unit, the meaning of this phrase is totally different from it is now, hence usually we could add n words as a feature unit into the vector to form the N-gram model. </li>
<li>Topic Model</li>
<li>Word Embedding: word embedding is a family of word vector models, the main idea is to project each word to a low-dimensional space (K = 50 -300 dimensions) using a dense vector. Each dimension in K-dimension would be viewed as a implicit topic. 
<span style="background-color: #FEE9E7">In general, in shallow learning models (traditional ML models), a good feature engineering step can help extremely good performance. Deep learning on the other hand, could help us with an automated feature engineering way via hidden layers. Hence, it makes sense for the deep learning model to beat the shallow learning model in general. Recurrent neural network and convolutional neural network are both good at capture the characteristics of the text while lowering the number of parameters that the model needs to learn, which can expedite the speed of training and also lower the risk of overfitting. </span></li>
</ul>
<h3 id="word2vec">Word2Vec<a class="headerlink" href="#word2vec" title="Permanent link">&para;</a></h3>
<p>One of the most common word embedding models, it is actually a shallow neural network. It can be of two different types of structures: 
1. Continuous Bag of Words
2. Skip-gram</p>
<h4 id="how-does-word2vec-work-what-is-the-difference-between-word2vec-and-lda-latent-dirichlet-allocation">How does word2vec work? what is the difference between word2vec and LDA (Latent Dirichlet allocation)<a class="headerlink" href="#how-does-word2vec-work-what-is-the-difference-between-word2vec-and-lda-latent-dirichlet-allocation" title="Permanent link">&para;</a></h4>
<ul>
<li>Continuous Bag of Words<ul>
<li>Goal is to use contextual words that predict the probability of the current word to appear.</li>
<li>Structure: <ul>
<li>input layer: w(t-2), w(t-1), ..., w(t+1), w(t+2) using one-hot encoding</li>
<li>projection/hidden layer: sum(probability)</li>
<li>output layer: w(t) using softmax</li>
</ul>
</li>
</ul>
</li>
<li>Skip-gram<ul>
<li>Goal is to use the current word to predict the probability of each contextual word.</li>
<li>Structure:<ul>
<li>input layer: w(t) using one-hot encoding</li>
<li>projection/hidden layer</li>
<li>output layer: w(t-2), w(t-1), ..., w(t+1), w(t+2) using softmax</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="image-data-not-sufficient-cold-start">Image Data not sufficient - Cold Start<a class="headerlink" href="#image-data-not-sufficient-cold-start" title="Permanent link">&para;</a></h3>
<p>When doing machine learning modeling, one very big problem that everyone may face would be not sufficient training data. </p>
<h4 id="what-would-you-do-if-the-training-data-is-not-sufficient-how-to-mitigate-this-issue">What would you do if the training data is not sufficient? How to mitigate this issue?<a class="headerlink" href="#what-would-you-do-if-the-training-data-is-not-sufficient-how-to-mitigate-this-issue" title="Permanent link">&para;</a></h4>
<p>Information that a model can provide include 1) information from training and 2) heuristic information that people provide from model formation (including design / learn / deduct). When training data not enough, it means that the model lacks information from training data, but need more a priori. a priori can be effective on models, including certain internal structure of the model, assumption or constraints. a priori can also be applied to datasets, for example using certain assumption to change / tune or expand the training data so it contains more effective information, which can facilitate model training and learning.</p>
<h3 id="overfitting">Overfitting<a class="headerlink" href="#overfitting" title="Permanent link">&para;</a></h3>
<p>One big problem that comes <strong>from not enough data is overfitting</strong>, which is that the model performs well on training set but the evaluation / prediction set is not good. The treatment can come from two different categories:
- methods based on models that decrease the risk of overfitting
    - simplify model - downgrade from non-linear to linear model
    - apply constraints to shrink hypothesis space - L1 / L2 regularization
    - integrated training
    - dropout hyperparameters
- data augmentation: manipulating data to expand the data set
    - image space manipulation
        - rotation / shrinkage / expansion / crop of the original image when working with image data
        - addition of noise to the image
        - color change of image
        - hue / contract / brightness of image
    - image feature engineering / extraction
        - data expansion or #over-sampling via SMOTE (Synthetic Minority Over-sampling Technique)
        - using GAN or other generative methods for good samples
    - transfer learning from other models and data
        - using pre-trained general model from big dataset, we could fine-tune specifically using the small datasets</p>
<h2 id="model-evaluation">Model Evaluation<a class="headerlink" href="#model-evaluation" title="Permanent link">&para;</a></h2>
<h3 id="evaluation-metrics-and-their-limitations">Evaluation metrics and their limitations<a class="headerlink" href="#evaluation-metrics-and-their-limitations" title="Permanent link">&para;</a></h3>
<p>When doing model evaluation, the classification / sort / regression problems seems to always use different metrics for evaluation. </p>
<h4 id="accuracy-and-its-limitations">Accuracy and its limitations<a class="headerlink" href="#accuracy-and-its-limitations" title="Permanent link">&para;</a></h4>
<p>The accuracy only measures the number of correct labels divided by the number of total labels. This can potentially lead to a issue <strong>when the number of labels are limited in the dataset</strong>. When negative samples composed 99% of the data, if every label is a negative one, we still get 99% accuracy. So, if we use more effective mean accuracy that quantifies the mean accuracy under each category, it would be a better metrics to work with.</p>
<h4 id="precision-recall-and-their-balance">Precision &amp; Recall and their balance<a class="headerlink" href="#precision-recall-and-their-balance" title="Permanent link">&para;</a></h4>
<h5 id="concept-of-precision-recall">Concept of Precision &amp; Recall<a class="headerlink" href="#concept-of-precision-recall" title="Permanent link">&para;</a></h5>
<p>Now we need to introduce the concept of precision and recall. 
Precision cares about the correctness of positive predictions, whereas recall cares about coverage of actual positives.  <span style="background-color: #FEE9E7">Precision and recall trade off via the decision threshold.</span> In a binary classification problem:
$$\text{Precision} = \frac{N_{\text{true positive}}}{N_{\text{true positive}} + N_{\text{false positive}}} = \frac{N_{\text{true positive}}}{N_{\text{positive predictions}}} $$</p>
<p>$$\text{Recall} = \frac{N_{\text{true positive}}}{N_{\text{true positive}} + N_{\text{false negative}}} = \frac{N_{\text{true positive}}}{N_{\text{actual positives}}} $$
The F1 score is their harmonic mean:
$$\text{F1} = \frac{2(\text{Precision})(\text{Recall})}{\text{Precision} + \text{Recall}} = \frac{2N_{\text{true positive}}}{2N_{\text{true positive}}+N_{\text{false positive}}+N_{\text{false negative}}} $$
this value ranges from 0 to 1 and penalizes imbalance, thus when either precision or recall is low, F1 drops sharply. <span style="background-color: #FEE9E7"> F1 should be used when false positives and false negatives matter about equally, especially with imbalanced classes. </span></p>
<h5 id="confusion-matrix-implementation">Confusion Matrix Implementation<a class="headerlink" href="#confusion-matrix-implementation" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code>import numpy as np

true_labels = np.array([0, 0, 1, 1, 0, 1, 0, 1, 1, 1])
predicted_labels = np.array([0, 1, 0, 1, 0, 1, 1, 1, 1, 0])

TP = np.sum((predicted_labels == 1) &amp; (true_labels == 1))
TN = np.sum((predicted_labels == 0) &amp; (true_labels == 0))
FP = np.sum((predicted_labels == 1) &amp; (true_labels == 0))
FN = np.sum((predicted_labels == 0) &amp; (true_labels == 1))

print(&quot;Confusion Matrix:\n TP: &quot;, TP, &quot;\tFP: &quot;, FP, &quot;\n FN: &quot;, FN, &quot;\tTN: &quot;, TN)

&#39;&#39;&#39;Output:
Confusion Matrix:
 TP:  4     FP:  2 
 FN:  2     TN:  2
&#39;&#39;&#39;
</code></pre></div>
<h5 id="precision-recall-in-ranking-retrieval-variants">Precision &amp; Recall in Ranking / retrieval variants<a class="headerlink" href="#precision-recall-in-ranking-retrieval-variants" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code>def precision_at_k(ground_truth_set, ranked_list, k):
    return len(set(ranked_list[:k]) &amp; ground_truth_set) / k
</code></pre></div>
<p><div class="highlight"><pre><span></span><code># when there are more than one query / user / example that we would like to test on our predictions, we use the weighted average of the precision_at_k.
def mean_precision_at_k(ground_truth_sets, ranked_lists, k):
    # ground_truth_sets and ranked_lists are aligned lists
    return sum(precision_at_k(g, r, k) for g, r in zip(ground_truth_sets, ranked_lists)) / len(ground_truth_sets)
</code></pre></div>
- <strong>Precision@k</strong> for <strong>one</strong> case <span class="arithmatex">\(q\)</span> (one list).
- <strong>Mean Precision@k</strong> average of those values over <strong>all</strong> cases <span class="arithmatex">\(q \in Q\)</span>.</p>
<p><strong>Example</strong>: when dealing with video vague search functionality, it seems that the search ranking model can return the top 5 precision pretty high, however, the user in reality still cannot find the videos they want, especially those unpopular ones. Where does this problem coming from?</p>
<p><strong>Root cause analysis</strong>: Coming back to the example above, the top 5 precision being really high, meaning that the model can get the true positive results on a pretty good level with a certain set of positive predictions; however, when it comes down to cases where users would like to find not so popular videos, the precision of ranks can be rather no so useful as the user is looking for not so well-defined labels, hence the good precision of popular videos would not be helpful for this case as <span style="background-color: #FEE9E7">model is not providing all the relevant videos to the user and this is a problem of not so good recall rate. </span> Let's say for the top 5 results, the precision@5 to be 100%, meaning that the correctness of the positive results is pretty higher, however, the recall@5 can still be 5%, meaning that only predicted 5 true positives although there are 100 actual positives involved. When doing model evaluation, it means that we should be focusing on both precision and recall, and also using different top N values for observations. </p>
<p>Hence, in general, when people evaluate the goodness of a sort algorithm, they also look at the P-R curve, where in this curve, the x-axis corresponds to recall rate whereas the y-axis corresponds to precision rate. </p>
<h5 id="use-of-p-r-curve-for-model-evaluation-and-threshold-choice">Use of P-R Curve for model evaluation and threshold choice<a class="headerlink" href="#use-of-p-r-curve-for-model-evaluation-and-threshold-choice" title="Permanent link">&para;</a></h5>
<p><img alt="p-r_curve" src="../resources/p-r_curve.png" />
Each data point on the curve corresponds to a precision-recall combination at a certain threshold for True samples of choice, for example 0.95 / 0.9, etc. The closer to the origin (0,0) point, the bigger the threshold is.</p>
<h5 id="how-to-pick-the-threshold-in-practice">How to pick the threshold in practice<a class="headerlink" href="#how-to-pick-the-threshold-in-practice" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Capacity-constrained:</strong> If reviewers can handle 300 cases/day, pick the smallest threshold that yields Ã¢â€°Ë†300 flags/day; report the resulting (Precision, Recall).</li>
<li><strong>Recall target:</strong> If policy demands <strong>Ã¢â€°Â¥95% recall</strong>, choose the lowest threshold achieving that, then report precision (and expected review load).</li>
<li><strong>Cost-based:</strong> Minimize <span class="arithmatex">\(\text{Cost}_{\text{false positives}}\cdot{\text{False Positives}}+\text{Cost}_{\text{false negatives}}\cdot{\text{False Negatives}}\)</span> over thresholds.
Also report <strong>AUPRC</strong> to compare models independent of a single threshold (higher is better, especially with class imbalance).</li>
</ul>
<h4 id="root-mean-squared-errors-rmse">Root-mean Squared Errors (RMSE)<a class="headerlink" href="#root-mean-squared-errors-rmse" title="Permanent link">&para;</a></h4>
<div class="arithmatex">\[ RMSE = \sqrt{\frac{\sum_{i=1}^{n}{(y_i - \hat y_i)^2}}{n}} \]</div>
<p>Root-mean squared error has long been used as the metric for evaluating the regression model.</p>
<p><strong>Example</strong>: as a streaming company, one would say that prediction of traffic for each series can be really important when it comes down to ads bidding and user expansion. One would like to use a regression model to predict the traffic trend of a certain series, but whatever regression model that one uses, the RMSE metric ends up being really high. But, in reality, the model 95% of the time predict error is less than 1%, with really good prediction results. What might be the reason of this extraordinarily good results?</p>
<p><strong>Root cause analysis</strong>: From what the example, says there are two possible ways for the RMSE to be ineffective: 1) n being really small hence at this moment, the calculated error cannot be measurable anymore, 2) all the errors between actual value and predicted value are over- / under-predicting that the summation at the end being really high, however, in reality it is not the case and <span style="color: #FF6961">3) one outlier being really off when comparing with other data points, it is contaminating the RMSE to be really big. </span> Coming back to the question, as 95% of the time to model has really good prediction error hence it means the other 5% of the time the model can be really off with big outliers and it could happen when a series with small traffic / newly come-out / newly accoladed could produce this big error.</p>
<p><strong>How to solve:</strong> 1) When we think these outliers are noises, then we need to filter them out at the early stage when doing data cleaning, 2) If we do not think they are noises, then we need to further improve the prediction capability of our algorithm so that we could somehow model the formation of these outliers. and 3) We could also use a better metric for the model evaluation. There are indeed better evaluation metrics that are of better robustness than RMSE, for example, Mean Absolute Percentage Error (MAPE):</p>
<h4 id="mean-absolute-percentage-error">Mean Absolute Percentage Error<a class="headerlink" href="#mean-absolute-percentage-error" title="Permanent link">&para;</a></h4>
<div class="arithmatex">\[MAPE = \sum_{i=1}^n{|\frac{(y_i - \hat y_i)}{y_i}|\cdot\frac{100}{n}}\]</div>
<p>When comparing with RMSE, MAPE normalizes the error rate of each data point to mitigate the outlier impact from the absolute error.</p>
<h4 id="expanding-on-the-regression-evaluation-metrics">Expanding on the regression evaluation metrics<a class="headerlink" href="#expanding-on-the-regression-evaluation-metrics" title="Permanent link">&para;</a></h4>
<h5 id="quick-definitions">Quick definitions<a class="headerlink" href="#quick-definitions" title="Permanent link">&para;</a></h5>
<p>Let <span class="arithmatex">\(y\)</span> be the true value and <span class="arithmatex">\(\hat y\)</span>Ã¢â‚¬â€¹ the prediction.  <br />
<strong>sMAPE</strong> (common form): 
<span class="arithmatex">\(<span class="arithmatex">\(\frac{100}{n}\sum\frac{2|y-\hat y|}{|y|+|\hat y|}\)</span>\)</span></p>
<h5 id="when-to-use-which">When to use which<a class="headerlink" href="#when-to-use-which" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Use RMSE</strong> when:<ul>
<li>Big errors are much worse than small ones (squared penalty).</li>
<li>The target never hits zero/near-zero and units are meaningful (e.g., dollars, Ã‚Â°C).</li>
<li>You care about <em>calibration</em> and smooth optimization (differentiable).</li>
</ul>
</li>
<li><strong>Use MAPE</strong> when:<ul>
<li>Stakeholders want an average <strong>percentage</strong> error that is easy to read.</li>
<li>True values are <strong>strictly positive and not near zero</strong> (e.g., revenue, demand &gt; 0).</li>
<li>You're okay that over-forecasts and under-forecasts are weighted differently (MAPE tends to penalize under-forecasting less when <span class="arithmatex">\(y\)</span> is small).</li>
</ul>
</li>
<li><strong>Use sMAPE</strong> when:<ul>
<li>You want a percentage-like metric that is <strong>less explosive near zero</strong> than MAPE.</li>
<li>You have occasional zeros or tiny values.</li>
<li>You accept that sMAPE has its own quirks (bounded but not perfectly symmetric in practice).</li>
</ul>
</li>
</ul>
<h5 id="strengths-gotchas-tldr">Strengths &amp; gotchas (TL;DR)<a class="headerlink" href="#strengths-gotchas-tldr" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>RMSE</strong><ul>
<li>Sensitive to large mistakes (good if that matches cost).</li>
<li>Outlier-heavy data can dominate the score.</li>
<li>Scale-dependent hard to compare across series with different scales.</li>
</ul>
</li>
<li><strong>MAPE</strong><ul>
<li>Intuitive (%).</li>
<li>Undefined at y=0; huge when y ~ 0.</li>
<li>Can favor <strong>under-forecasting</strong> for small y.</li>
</ul>
</li>
<li><strong>sMAPE</strong><ul>
<li>Handles zeros better; bounded.        </li>
<li>Still quirky near zero and not a true solution for optimization.</li>
<li>Different papers/tools use slightly different variants</li>
</ul>
</li>
</ul>
<h5 id="other-basic-metrics-you-should-know">Other basic metrics you should know<a class="headerlink" href="#other-basic-metrics-you-should-know" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>MAE</strong>: Robust to outliers vs RMSE; easy to explain (units).</li>
</ul>
<h5 id="simple-decision-guide">Simple decision guide<a class="headerlink" href="#simple-decision-guide" title="Permanent link">&para;</a></h5>
<ol>
<li><strong>Zeros or tiny targets?</strong><ul>
<li>Avoid plain MAPE. Prefer <strong>sMAPE</strong></li>
</ul>
</li>
<li><strong>Large errors are very costly?</strong><ul>
<li>Use <strong>RMSE</strong> (or set a business-weighted loss).</li>
</ul>
</li>
<li><strong>Need % interpretability across series?</strong><ul>
<li>Use <strong>sMAPE</strong>, or <strong>MASE</strong> (if comparing to a baseline).</li>
</ul>
</li>
<li><strong>Care about relative ratios?</strong><ul>
<li>Use <strong>RMSLE/MSLE</strong> (with positive targets).</li>
</ul>
</li>
<li><strong>Mixed scales or many series?</strong><ul>
<li><strong>WAPE</strong> or <strong>MASE</strong> are safe, comparable choices.</li>
</ul>
</li>
</ol>
<h5 id="practical-tips">Practical tips<a class="headerlink" href="#practical-tips" title="Permanent link">&para;</a></h5>
<ul>
<li>If you must report a % and have zeros, say: We use sMAPE (formula shown) instead of MAPE to handle zeros; we also report WAPE for scale-free comparability.</li>
<li>Always <strong>state the exact formula</strong> you use (especially for sMAPE) to avoid confusion.</li>
<li><span style="color:#FF6961">Consider reporting two metrics: one business-facing (% like WAPE/sMAPE) + one technical (MAE/RMSE).</span></li>
</ul>
<p>Overall, one should always report a pair / set of MECE metrics to evaluate their algorithms to better understand &amp; discover the problems in the model, to better solve cases in real business settings.</p>
<h4 id="roc-curves">ROC Curves<a class="headerlink" href="#roc-curves" title="Permanent link">&para;</a></h4>
<p>Binary classifiers are the mostly used and applied classifier in the ML industry. There are a lot of different metrics that one could use for evaluate the binary classifiers, including precision, recall, F1 score and P-R curve. But these metrics are only reflecting one aspect of the model. Hence, ROC curves can be of really good use. </p>
<h5 id="what-is-a-roc-curve">What is a ROC curve<a class="headerlink" href="#what-is-a-roc-curve" title="Permanent link">&para;</a></h5>
<p>ROC curves are called receiver Operating Characteristic Curves, which established from the military field and are often used in the medical industry as well. This curve's x-axis is the false positive rate, whereas the y-axis is the true-positive rate. </p>
<p><span class="arithmatex">\(<span class="arithmatex">\(\text{False Positive Rate} = \frac{\text{False Positive}}{\text{Negative}}\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(\text{True Positive Rate} = \frac{\text{True Positive}}{\text{Positive}}\)</span>\)</span>
<strong>Example</strong>: There are 10 patients, where in there are 3 positive cancer patients, and the rest are negative patients. The hospital decides to do diagnosis on these customers and figured that 2 are true positive cancer patients. In this case:</p>
<div class="arithmatex">\[\text{False Positive Rate} = \frac{\text{False Positive}}{\text{Negative}} = \frac{1}{7}$$
$$\text{True Positive Rate} = \frac{\text{True Positive}}{\text{Positive}}=\frac{2}{3}\]</div>
<h5 id="how-to-draw-a-roc-curve">How to draw a ROC curve<a class="headerlink" href="#how-to-draw-a-roc-curve" title="Permanent link">&para;</a></h5>
<ul>
<li>What is needed<ul>
<li>True labels <span class="arithmatex">\(y \in \{0,1\}\)</span></li>
<li>A <strong>score</strong> for the positive class per item (probability or decision score).  </li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>Sample Number</th>
<th>True Label</th>
<th>Model Output Probability as Positive</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Positive</td>
<td>0.9</td>
</tr>
<tr>
<td>2</td>
<td>Positive</td>
<td>0.8</td>
</tr>
<tr>
<td>3</td>
<td>Negative</td>
<td>0.7</td>
</tr>
</tbody>
</table>
<p>From this example, we could then plot out the true positive rate (TPR) as the x-axis and false positive rate (FPR) as the y-axis for the curve, hence getting the ROC curve. There is a more direct way to plot the ROC curve as well:</p>
<ul>
<li>Getting the number of Positive &amp; Negative samples, i.e. assuming number of positive samples to be P and negative to be N.</li>
<li>Getting the x-axis labels to be the count of negative samples, and y-axis labels to be the count of positive samples, then use the model output probability to do sorting of the samples</li>
<li>Now draw the ROC curve from origin, whenever seeing a positive sample to draw a vertical line segment of +1 increment on y-axis, whenever seeing a negative sample then we draw a horizontal line segment along the x-axis until we reach the final sample with curve ending at (1,1).</li>
</ul>
<div class="highlight"><pre><span></span><code>from matplotlib import pyplot as plt
from numpy import random

truth_labels = [1 if random.rand() &gt; 0.6 else 0 for _ in range(500)]
# we generate some random predictions that would normally be obtained from the model
# If a predicted probability is higher than the threshold, it is considered to be a positive outcome 
predicted_probs = [max(0, min(1, random.normal(loc=label, scale=0.3))) for label in truth_labels]

def roc_curve(truth_labels, predicted_probs):
    thresholds = [0.1 * i for i in range(11)]
    tprs, fprs = [], []
    for threshold in thresholds:
        tp = fp = tn = fn = 0  # initialize confusion matrix counts
        # for each prediction
        for i in range(len(truth_labels)):
            # calculate confusion matrix counts
            if predicted_probs[i] &gt;= threshold:
                if truth_labels[i] == 1:
                    tp += 1
                else:
                    fp += 1
            else:
                if truth_labels[i] == 1:
                    fn += 1
                else:
                    tn += 1
        # track the TPR and FPR for this threshold
        tprs.append(tp / (tp + fn))  # True Positive Rate (TPR)
        fprs.append(fp / (tn + fp))  # False Positive Rate (FPR)
    return tprs, fprs


tprs, fprs = roc_curve(truth_labels, predicted_probs)
plt.plot(fprs, tprs, marker=&#39;.&#39;)
plt.show()
</code></pre></div>
<h5 id="how-to-calculate-the-auc-area-under-curve">How to calculate the AUC (area under curve)?<a class="headerlink" href="#how-to-calculate-the-auc-area-under-curve" title="Permanent link">&para;</a></h5>
<p>As simple as it could be, AUC is the area under the ROC curve, which can quantitatively reflect the model performance based on ROC curve. It is simple to calculate AUC along RUC x-axis. Due to that ROC curve tends to be above y=x, AUC values are usually between 0.5-1. The bigger the AUC is, the better the classifier is as the more likely that the classifier put the true positive samples at the front. </p>
<div class="highlight"><pre><span></span><code>def compute_aucroc(tprs, fprs):
    aucroc = 0
    for i in range(1, len(tprs)):
        aucroc += 0.5 * abs(fprs[i] - fprs[i - 1]) * (tprs[i] + tprs[i - 1])
    return aucroc

aucroc = compute_aucroc(tprs, fprs)
print(f&quot;The AUC-ROC value is: {aucroc}&quot;)  # The AUC-ROC value is: 0.9827272125066242
</code></pre></div>
<p>We have touched on the P-R curve for evaluating classification or sort algorithms. Comparing with P-R curve, there is one important character of ROC curve, which is that when positive / negative sample distribution change significant, the ROC curve shape could stay rather consistently whereas the P-R curve shape would be changing. This makes the ROC curve to mitigate the interference from diverse test sets and could more objectively evaluate the algorithm. In reality, when positive counts are much less than the negative counts, when switching dataset the data can be of big change, so a stable and robust evaluation would be important. Hence, usually ROC can be used in more variety of scenarios and could be utilized in sort / recommendation / ads. </p>
<h5 id="what-each-curve-shows">What each curve shows<a class="headerlink" href="#what-each-curve-shows" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>ROC</strong>: y = True Positive Rate (recall), x = False Positive Rate.<br />
<em>"How well do I separate positives from negatives overall?"</em>
    _"If I take the items my model flags as positive, how many are actually positive?</li>
<li><strong>PR</strong>: y = Precision, x = Recall.<br />
<em>"When I go after positives, how clean are my catches?"</em>
    _"As I move the threshold, how well do I trade off catching positives vs accidentally flagging negatives?"</li>
</ul>
<h5 id="when-to-use-which_1">When to use which<a class="headerlink" href="#when-to-use-which_1" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Use PR (Precision &amp; Recall) when positives are rare or review capacity is limited.</strong><br />
    Examples: fraud, disease screening, anomaly detection, search/retrieval, human-in-the-loop queues.<br />
    Why: PR focuses on the <em>quality of retrieved positives</em>. Baseline matters: random <strong>AUPRC prevalence</strong> (e.g., 1% positives random AUPRC = 0.01).</li>
<li><strong>Use ROC when classes are roughly balanced or you care about both error types evenly.</strong><br />
    Examples: many general classifiers, spam vs ham with moderate prevalence, A/B classifiers in balanced datasets.<br />
    Why: ROC is insensitive to class imbalance and summarizes ranking quality across thresholds. Random <strong>AUC-ROC = 0.5</strong>.</li>
</ul>
<h5 id="intuition-about-imbalance">Intuition about imbalance<a class="headerlink" href="#intuition-about-imbalance" title="Permanent link">&para;</a></h5>
<ul>
<li>With 1,000,000 negatives and 1,000 positives, an FPR of <strong>0.5%</strong> looks tiny on ROC, but it's <strong>5,000 false alarms</strong> precision will be poor.<br />
    PR makes this visible; ROC can look deceptively great.</li>
</ul>
<h5 id="how-to-choose-in-practice">How to choose in practice<a class="headerlink" href="#how-to-choose-in-practice" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Rare positives or ops-constrained?</strong> Prefer <strong>PR</strong> (and report Precision/Recall at your operating threshold or <strong>Precision@k</strong>).</li>
<li><strong>Balanced costs/distribution?</strong> <strong>ROC</strong> is fine (and stable).</li>
<li><strong>Comparing models broadly?</strong> Report <strong>both</strong> AUC-ROC and AUPRC, plus a point metric at your intended threshold.</li>
</ul>
<h5 id="reading-the-curves">Reading the curves<a class="headerlink" href="#reading-the-curves" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>ROC</strong>: closer to top-left is better; AUC near 1 is strong.</li>
<li><strong>PR</strong>: higher curve is better; sustaining high precision as recall grows is ideal.</li>
<li>Curves can <strong>cross</strong>. Pick the model thatÃ¢â‚¬â„¢s better in the <strong>recall region you care about</strong> (e.g., recall Ã¢â€°Â¥ 0.9). Consider <strong>partial AUC</strong> (ROC) or <strong>AUPRC over a recall range</strong>.</li>
</ul>
<h5 id="what-to-report-good-default">What to report (good default)<a class="headerlink" href="#what-to-report-good-default" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>AUPRC + AUC-ROC</strong> (global picture)</li>
<li><strong>(Precision, Recall)</strong> (or <span class="arithmatex">\(F_\beta\)</span>) at the <strong>chosen threshold</strong></li>
<li>If capacity-limited: <strong>Precision@k</strong> (and expected volume flagged)</li>
</ul>
<h4 id="use-of-cosine-distance">Use of cosine distance<a class="headerlink" href="#use-of-cosine-distance" title="Permanent link">&para;</a></h4>
<p>How to evaluate the distance between samples can also define the optimization target and training method. In ML problems, we usually take the features to be of vector form, so when analyzing the two feature vector similarity, we could use cosine similarity. The cosine similarity can range from -1 to 1, where when two vectors are exactly the same, the cosine similarity becomes 1. Hence, when looking at distances, 1-cosine similarity becomes the cosine distance. Overall, the cosine distance is [0,2] and the same two vectors their cosine distance becomes 0.</p>
<h5 id="definition-of-euclidean-distance-cosine-distance">Definition of Euclidean Distance &amp; Cosine Distance<a class="headerlink" href="#definition-of-euclidean-distance-cosine-distance" title="Permanent link">&para;</a></h5>
<p><strong>Euclidean Distance</strong>
For vectors <span class="arithmatex">\(x,y\in\mathbb{R}^d\)</span>:</p>
<div class="arithmatex">\[d_{\text{Euc}}(x,y)=\sqrt{\sum_{i=1}^{d}(x_i-y_i)^2} \in [0,\infty) \]</div>
<ul>
<li><strong>What it measures:</strong> straight-line (L2) distance in space.</li>
<li><strong>Sensitive to scale/magnitude:</strong> doubling a vector doubles distances.</li>
<li><strong>Squared form:</strong> sometimes use <span class="arithmatex">\(\|x-y\|^2\)</span> (no square root) for speed/convexity.~</li>
</ul>
<p><strong>Cosine Distance</strong>
Start with cosine <strong>similarity</strong>:</p>
<div class="arithmatex">\[\text{cos\_sim}(x,y)=\frac{x\cdot y}{\|x\|\,\|y\|}\in[-1,1]\]</div>
<p>Cosine <strong>distance</strong> (common definition):
<span class="arithmatex">\(<span class="arithmatex">\(d_{\text{cos}}(x,y)=1-\text{cos\_sim}(x,y)\in[0,2]\)</span>\)</span></p>
<ul>
<li><strong>What it measures:</strong> difference in <strong>direction</strong> (angle) only.</li>
<li><strong>Scale-invariant:</strong> multiplying a vector by a positive constant doesnÃ¢â‚¬â„¢t change it.</li>
</ul>
<p>Overall, on unit vectors, Euclidean and cosine distances are monotonic transforms.
Also, on a unit circle, one would see:
<span class="arithmatex">\(<span class="arithmatex">\(\|A-B\|=\sqrt{2(1-cos(A,B))}\)</span>\)</span>
- <strong>When to use which</strong>
    - Use <strong>Euclidean</strong> when magnitude matters (e.g., real spatial distances, continuous features with meaningful scales).
    - Use <strong>Cosine</strong> when orientation matters more than length (e.g., text/image embeddings, TF-IDF vectors).</p>
<h5 id="when-to-use-cosine-similarity-but-not-euclidean-distance">When to use cosine similarity but not Euclidean distance?<a class="headerlink" href="#when-to-use-cosine-similarity-but-not-euclidean-distance" title="Permanent link">&para;</a></h5>
<p>For two vectors A and B, when their cosine similarity are being defined as <span class="arithmatex">\(cos(A,B)=\frac{A\cdot B}{\|A\|_2 \|B\|_2}\)</span> , i.e. the cosine of angle between two vectors, we thus measure the angular distance between them, rather than the absolute magnitude, with the range being [-1,1]. When a pair of text being very different in length, but with similar content, if using Euclidean distance, one can think their distance being pretty big whereas when using cosine similarity, the angle between the two can be rather small, hence giving high similarity. In text, visual, video, image industries, when the objective has high dimensions, cosine can still retain its character of [-1,1] whereas the Euclidean distance number can be really big. </p>
<p><span style="color:#FF6961">Overall, Euclidean distance measures the absolute difference between numbers whereas the cosine distance measures the directional relative difference. </span> </p>
<p>Taking an example of measuring user behavior of watching two different TV series:
    - user A's watch vector = (0,1)
    - user B's watch vector = (1,0)
It is obvious that the cosine distance between the two can be really big whereas their Euclidean distance is small. </p>
<p>When measuring user A/B preference, we focus more on relative difference, hence we should be using the cosine distance whereas when we are analyzing user login frequency or activity, we should be using Euclidean distance instead as the cosine distance would think two users of vector (1,10) and (10,100) are more similar to each other.</p>
<h5 id="is-cosine-distance-a-strictly-defined-distance">Is cosine distance a strictly defined distance?<a class="headerlink" href="#is-cosine-distance-a-strictly-defined-distance" title="Permanent link">&para;</a></h5>
<p>No, it is not strictly defined as it satisfies the Non-negativity &amp; identity (strictness), symmetry but does not satisfy the triangle inequality. A use case of this question is that when reading the word vector of <code>comedy</code> and <code>funny</code> and also <code>happy</code> and <code>funny</code>, their cosine distance is &lt; 0.3, whereas the distance between <code>comedy</code>and <code>happy</code> is 0.7. </p>
<h3 id="model-evaluation-methods">Model Evaluation Methods<a class="headerlink" href="#model-evaluation-methods" title="Permanent link">&para;</a></h3>
<p>In ML algorithm design, we usually split the samples into training and test data set, where the training set is used to training the model and the test set is used to evaluate the model. In sample split and model evaluation process, we could use different sampling or evaluation methods. </p>
<h4 id="in-model-evaluation-what-are-the-main-evaluation-methods-what-are-their-pros-and-cons">In model evaluation, what are the main evaluation methods, what are their pros and cons?<a class="headerlink" href="#in-model-evaluation-what-are-the-main-evaluation-methods-what-are-their-pros-and-cons" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Holdout evaluation</strong>: Holdout evaluation is the easiest way as it randomly split the original sample set into training and evaluation. For example, for a clickthrough rate prediction algorithm, we split the samples into 70 - 30%. We use the 70% data for model training and the 30% for evaluation, including ROC curve, accuracy calculation and recall rate metric evaluation. This has significant downside: the calculated final evaluation metric is highly correlated with the original data split. In order to eliminate this randomness, researchers started to use the "cross validation" idea.</li>
<li><strong>cross-validation</strong>: k-fold cross validation would always split the data set into k different sets that are of same counts. The method goes through all the k sample sets and always use the current subset as the evaluation set whereas the other ones are training set. usually we use k = 10.</li>
<li><strong>Bootstrap</strong>: <ul>
<li>Make a <strong>fake test set</strong> by randomly picking the same number of rows from your real test set <strong>with replacement</strong> (so rows can repeat and some are left out).<ul>
<li>Suppose the test set has <strong>n rows</strong>.</li>
<li>Pick <strong>n indices at random WITH replacement</strong> from <code>0..n-1</code>. (Duplicates allowed; some rows won't be picked.)</li>
<li>Those picked rows form one <strong>fake test set</strong>. </li>
</ul>
</li>
<li>On that fake set, compute your metric (accuracy, F1, AUC, RMSE whatever you care about).</li>
<li>Repeat steps 1-2 a lot (like <strong>1,000 times</strong>).</li>
<li>Now you have 1,000 metric values.<ul>
<li>The <strong>average</strong> is your central estimate.</li>
<li>The <strong>middle 95% range</strong> (ignore the lowest 2.5% and highest 2.5%) is your <strong>95% confidence interval</strong>.
As <span class="arithmatex">\(n\)</span> gets large, about <strong>36.8%</strong> of items are not in the set (never selected) and <strong>63.2%</strong> appear at least once. This is the source of the bootstrap terminology</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="hyperparameter-tuning">Hyperparameter tuning<a class="headerlink" href="#hyperparameter-tuning" title="Permanent link">&para;</a></h3>
<p>For a lot of algorithm engineers, hyperparameter tuning can be really of headache, as there is no other way other than empirically tune the parameters to a reasonable range, while it is really important for the algorithm to be effective.</p>
<h5 id="what-are-some-of-the-common-ways-of-hyperparameter-tuning">What are some of the common ways of hyperparameter tuning?<a class="headerlink" href="#what-are-some-of-the-common-ways-of-hyperparameter-tuning" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>grid search</strong>: Exhaustive on a small, <strong>low-dimensional</strong> space. Deterministic but expensive; scales poorly. In reality, it tend to be used as a bigger search space and larger step size to find the possible range of optimal results, then to shrink the search space and find more accurate optimal solution.</li>
<li><strong>random search</strong>: Sample hyperparams at random (often <strong>log-uniform</strong> for learning rates). Much better than grid when only a few dims matter but cannot guarantee for a optimal solution.</li>
<li><strong>Bayesian optimization</strong>: Model config -&gt;score to pick promising next trials. Unlike random/grid search <strong>do not learn</strong> from past trials, BO <strong>uses what you have learned so far</strong> to place the next (expensive) trial where it is most likely to pay off.</li>
</ul>
<h3 id="overfit-and-underfit">Overfit and Underfit<a class="headerlink" href="#overfit-and-underfit" title="Permanent link">&para;</a></h3>
<p>This section tells how one could efficiently recognize overfit and underfit scenarios and do model improvements based on what has been identified. </p>
<h5 id="what-is-overfit-and-what-is-underfit">What is overfit and what is underfit?<a class="headerlink" href="#what-is-overfit-and-what-is-underfit" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Overfit</strong> means that a model can be overfitting on its training data whereas on the test and new data sets, it's performing worse. </li>
<li><strong>Underfit</strong> means that the model is performing illy on both training and test data sets. </li>
</ul>
<h5 id="what-are-some-ways-to-mitigate-the-overfit-and-underfit">What are some ways to mitigate the overfit and underfit?<a class="headerlink" href="#what-are-some-ways-to-mitigate-the-overfit-and-underfit" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Avoid overfit</strong>: <ul>
<li><strong>Data</strong>: obtaining more data is one primitive way of solving overfit problem as more data can help the model to learn more efficient features to mitigate the impact from noise. Using rotation or expansion for image or GAN for getting more new training data.</li>
<li><strong>Model</strong>: one could use less complicated / complex model to avoid overfitting. For example, in NN one could reduce the number of layers or neurons in each layer; or in decision tree, one could reduce the depth of the tree or cut the tree.</li>
<li><strong>Regularization</strong>: one could use L2 regularization in model parameters to constraint the model. </li>
<li><strong>ensemble method</strong>: ensemble method is to integrate multiple models together to avoid a single model overfitting issue, such as bagging methods.</li>
</ul>
</li>
<li><strong>Avoid underfit</strong>:<ul>
<li>add more features: when there is not enough features or the features are not relevant with the sample labels, there would be a underfit. We could dig into contextual features / ID features / combination of features to obtain better results. In deep learning, factor decomposition / gradient-boosted decision tree / deep-crossing can all be used for get more features.</li>
<li>increase the complexity of model. </li>
<li>decrease regularization parameters. </li>
</ul>
</li>
</ul>
<h3 id="early-stoppings">Early Stoppings<a class="headerlink" href="#early-stoppings" title="Permanent link">&para;</a></h3>
<ul>
<li>Early stopping watches <strong>validation</strong> loss/metric and halts training when it stops improving, and is a <strong>stopping rule</strong> driven by the <strong>validation metricâ€™s change</strong>, not a pre-fixed iteration count</li>
<li>It <strong>reduces overfitting</strong> (lower variance) by not letting the model memorize noise; acts like <strong>implicit L2</strong> regularization.
Train while checking performance on a validation set. Whenever the validation score improves, remember those weights. If it doesnâ€™t improve for a while (patience), stop and roll back to the best checkpoint. This caps model complexity at the point where it generalized best, preventing the later epochs from fitting noise</li>
</ul>
<h3 id="l2-l1-regularization">L2 / L1 Regularization<a class="headerlink" href="#l2-l1-regularization" title="Permanent link">&para;</a></h3>
<h4 id="setup">Setup<a class="headerlink" href="#setup" title="Permanent link">&para;</a></h4>
<p>Model (no intercept for simplicity):</p>
<div class="arithmatex">\[\hat y_i = w\,x_i\]</div>
<p><strong>Data loss</strong> (sum of squared errors):</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(\sum_i (y_i - w x_i)^2\)</span>\)</span>
<strong>L2-regularized loss</strong> (ridge):
<span class="arithmatex">\(<span class="arithmatex">\(\underbrace{\sum_i (y_i - w x_i)^2}_{\text{fit the data}} \;+\; \underbrace{\lambda\, w^2}_{\text{penalize big weights}}\)</span>\)</span>
- <span class="arithmatex">\(\lambda&gt;0\)</span> controls the strength of the penalty (larger <span class="arithmatex">\(\lambda\)</span> stronger shrinkage).
- In practice, we usually <strong>don't penalize the bias/intercept</strong>.</p>
<h4 id="how-l2-penalizes-the-parameter">How L2 Penalizes the Parameter<a class="headerlink" href="#how-l2-penalizes-the-parameter" title="Permanent link">&para;</a></h4>
<p>Take derivative w.r.t. <span class="arithmatex">\(w\)</span> and set to 0:</p>
<div class="arithmatex">\[\frac{\partial}{\partial w}\Big[\sum_i (y_i - w x_i)^2 + \lambda w^2\Big] = -2\sum_i x_i(y_i - w x_i) + 2\lambda w = 0\]</div>
<p>Rearrange:
<span class="arithmatex">\(<span class="arithmatex">\(w\big(\sum_i x_i^2 + \lambda\big) = \sum_i x_i y_i \quad\Rightarrow\quad \boxed{\,w_{\text{ridge}} = \dfrac{\sum_i x_i y_i}{\sum_i x_i^2 + \lambda}\,}\)</span>\)</span>
Compare to <strong>unregularized</strong> OLS:
<span class="arithmatex">\(<span class="arithmatex">\(w_{\text{OLS}} = \dfrac{\sum_i x_i y_i}{\sum_i x_i^2}\)</span>\)</span>
L2 adds <span class="arithmatex">\(\lambda\)</span> to the denominator and <strong>shrinks <span class="arithmatex">\(w\)</span> toward 0</strong>.</p>
<h4 id="why-l2-decrease-variance-and-increase-bias">Why L2 decrease variance and increase bias?<a class="headerlink" href="#why-l2-decrease-variance-and-increase-bias" title="Permanent link">&para;</a></h4>
<p>L2 regularization constrains how large the parameters can get. Constraining parameters makes the fitted function smoother/less wiggly, so predictions donâ€™t swing wildly when the training sample changesâ€”this cuts variance. The tradeoff is that the constrained model canâ€™t perfectly adapt to the true signal, so estimates are pulled toward zero (or toward simpler shapes), which introduces bias.</p>
<h4 id="tiny-numeric-example">Tiny Numeric Example<a class="headerlink" href="#tiny-numeric-example" title="Permanent link">&para;</a></h4>
<p>Data: <span class="arithmatex">\(x=[0,1,2,3]\)</span>, <span class="arithmatex">\(y=[0,1,2,60]\)</span> (last point is an outlier)
- <span class="arithmatex">\(\sum x_i^2 = 14, \sum x_i y_i = 185\)</span>
Weights:
- <strong>OLS (no L2):</strong> <span class="arithmatex">\(185/14 \approx 13.214\)</span>
- <strong>L2, <span class="arithmatex">\(\lambda=10\)</span>:</strong> <span class="arithmatex">\(185/(14+10) = 185/24 \approx 7.708185\)</span>
- <strong>L2, <span class="arithmatex">\(\lambda=100\)</span>:</strong> <span class="arithmatex">\(185/(14+100) = 185/114 \approx 1.623\)</span>
As <span class="arithmatex">\(\lambda\)</span> grows, <span class="arithmatex">\(w\)</span> is <strong>pulled toward 0</strong>, limiting the impact of the outlier.</p>
<h4 id="gradient-descent-view-weight-decay">Gradient-Descent View (Weight Decay)<a class="headerlink" href="#gradient-descent-view-weight-decay" title="Permanent link">&para;</a></h4>
<p>With learning rate <span class="arithmatex">\(\eta\)</span>:
<span class="arithmatex">\(<span class="arithmatex">\(w_{\text{new}} = w_{\text{old}} - \eta\Big(\underbrace{-2\sum_i x_i(y_i - w_{\text{old}} x_i)}_{\text{data gradient}} \;+\; \underbrace{2\lambda w_{\text{old}}}_{\text{L2 shrink}}\Big)\)</span>\)</span></p>
<p>The <span class="arithmatex">\(+2\lambda w\)</span> term is the <strong>shrinkage</strong> that steadily decays weights.</p>
<h4 id="multi-feature-form-for-reference">Multi-Feature Form (for reference)<a class="headerlink" href="#multi-feature-form-for-reference" title="Permanent link">&para;</a></h4>
<p>For features <span class="arithmatex">\(X\in \mathbb{R}^{n\times d}\)</span>, target <span class="arithmatex">\(\mathbf{y}\)</span>:</p>
<div class="arithmatex">\[\mathbf{w}_{\text{ridge}} = (X^\top X + \lambda I)^{-1} X^\top \mathbf{y}\]</div>
<h4 id="copy-paste-python">Copy-Paste Python<a class="headerlink" href="#copy-paste-python" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>import numpy as np

x = np.array([0,1,2,3], dtype=float)
y = np.array([0,1,2,60], dtype=float)

Sxx = np.sum(x**2)
Sxy = np.sum(x*y)

def ridge_weight(lmbda):
    return Sxy / (Sxx + lmbda)

print(&quot;w_OLS        =&quot;, Sxy / Sxx)
for lmbda in [10, 100]:
    print(f&quot;w_ridge&quot;, ridge_weight(lmbda))
</code></pre></div>
<p><strong>Notes</strong>
- Standardize features before using L2/L1 (esp. linear/logistic).
- Tune <span class="arithmatex">\(\lambda\)</span> via cross-validation.
- Do <strong>not</strong> penalize the bias term.</p>
<h2 id="classical-algorithms">Classical Algorithms<a class="headerlink" href="#classical-algorithms" title="Permanent link">&para;</a></h2>
<h3 id="linear-regression">Linear Regression<a class="headerlink" href="#linear-regression" title="Permanent link">&para;</a></h3>
<p>There are two central provinces in the world of regression: simple linear regression and multiple linear regression. </p>
<h4 id="formula-of-simple-linear-regression">Formula of Simple Linear Regression<a class="headerlink" href="#formula-of-simple-linear-regression" title="Permanent link">&para;</a></h4>
<p>The formula of linear regression can be represented as <span class="arithmatex">\(<span class="arithmatex">\(y=c+m\cdot x\)</span>\)</span>
The formula revolves around minimizing residuals. Imagine residuals as the distance between the actual and predicted values of the dependent variable <span class="arithmatex">\(y\)</span>:
<span class="arithmatex">\(<span class="arithmatex">\(m = \frac{\sum_{i=1}^N{(x_i-\bar x)(y_i-\bar y)}}{\sum_{i=1}^N(x_i-\bar x)^2}\)</span>\)</span>
and the constant corresponds to <span class="arithmatex">\(c=\bar y - m \cdot\bar x\)</span>. </p>
<div class="highlight"><pre><span></span><code>import numpy as np

# Step 1: Get the data set
x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 5, 4, 5])

# Step 2: Compute the mean of the X and y
mean_x = np.mean(x)
mean_y = np.mean(y)

# Step 3: Calculate the coefficients
m = np.sum((x - mean_x) * (y - mean_y)) / np.sum((x - mean_x) ** 2)
c = mean_y - m * mean_x

# Voila! We have our model
print(f&quot;Model: y = {c} + {m}*x&quot;) Â # Output: Model: y= 2.2 + 0.6*x
</code></pre></div>
<h4 id="formula-of-multiple-linear-regression">Formula of Multiple Linear Regression<a class="headerlink" href="#formula-of-multiple-linear-regression" title="Permanent link">&para;</a></h4>
<p>For simple linear regression formula, we have <span class="arithmatex">\(y=\beta_0 + \beta_1x\)</span>, for multiple linear regression, we add multiple independent variables <span class="arithmatex">\(x_1, x_2, ... , x_m\)</span>. Suppose we had n data points, each with m features, then X would be like: <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{X}=\begin{bmatrix}  
1 &amp; x_{1,1} &amp; x_{1,2} &amp; ... &amp; x_{1,m} \\  
1 &amp; x_{2,1} &amp; x_{2,2} &amp; ... &amp; x_{2,m} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n,1} &amp; x_{n,2} &amp; ... &amp; x_{n,m} \\
\end{bmatrix} \in \mathbb{R^{n\times (m+1)}}, \mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} \in \mathbb{R^{n\times 1}}, \mathbf{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_m \end{bmatrix} \in  \mathbb{R^{(m+1)\times 1}}\)</span>\)</span> Each row represents the m features for a single data point. The first column with <span class="arithmatex">\(\mathbf{1}\)</span>s are the bias / intercept of each equation. The normal equation would be of form
<span class="arithmatex">\(<span class="arithmatex">\(\beta = (X^T X)^{-1}X^Ty\)</span>\)</span>
The predicted <span class="arithmatex">\(\hat y\)</span> values can be represented as
<span class="arithmatex">\(<span class="arithmatex">\(\hat y = (1 \cdot \beta_0)+(\beta_1 \cdot x_1) + (\beta_2 \cdot x_2) + \dots + (\beta_m \cdot x_m)\)</span>\)</span>
To calculate all the predictions at once, we take the dot product of <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(\beta\)</span>:
$$\mathbf{y} = \begin{bmatrix} y_1 \ y_2 \ \vdots \ y_n \end{bmatrix} = X\cdot \beta =\begin{bmatrix}<br />
1 &amp; x_{1,1} &amp; x_{1,2} &amp; ... &amp; x_{1,m} \<br />
1 &amp; x_{2,1} &amp; x_{2,2} &amp; ... &amp; x_{2,m} \
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \
1 &amp; x_{n,1} &amp; x_{n,2} &amp; ... &amp; x_{n,m} \
\end{bmatrix} \begin{bmatrix} \beta_0 \ \beta_1 \ \vdots \ \beta_m \end{bmatrix} $$</p>
<h4 id="linear-regression-model-evaluation">Linear Regression Model Evaluation<a class="headerlink" href="#linear-regression-model-evaluation" title="Permanent link">&para;</a></h4>
<h5 id="coefficient-of-determination-r2-score">coefficient of determination (<span class="arithmatex">\(R^2\)</span> score)<a class="headerlink" href="#coefficient-of-determination-r2-score" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(<span class="arithmatex">\(R^2=1-\frac{SS_\text{residuals}}{SS_\text{total}} = 1 - \frac{\sum_{i=1}^n(y_i - \hat y_i)^2}{\sum_{i=1}^n(y_i - \bar y_i)^2}\)</span>\)</span> Where <span class="arithmatex">\(SS_\text{residuals}\)</span> denotes the residual sum of squares for predictions and <span class="arithmatex">\(SS_\text{total}\)</span> denotes the total sum of squares from actual values. A higher R-squared value / closer to 1 indicates a good model fit.</p>
<div class="highlight"><pre><span></span><code>import numpy as np
# given data
housing_data = np.array(\[\[1800, 3\], \[2400, 4\],\[1416, 2\], \[3000, 5\]\])
prices = np.array([350000, 475000, 230000, 640000])

# adding 1s to our matrix
# ones = np.ones(shape=(len(housing_data), 1))
# X = np.append(ones, housing_data, axis=1)
X = np.c_[np.ones((len(housing_data),1)),X] # add bias parameter to X

# calculating coefficients
coefficients = np.linalg.inv(X.T @ X) @ X.T @ prices

# predicting prices
predicted_prices = X @ coefficients

# calculating residuals
residuals = prices - predicted_prices

# calculating total sum of squares
sst = np.sum((prices - np.mean(prices)) ** 2)

# calculating residual sum of squares
ssr = np.sum(residuals ** 2)

# calculating R^2
r2 = 1 - (ssr/sst)

print(&quot;Coefficients:&quot;, coefficients)
print(&quot;Predicted prices:&quot;, predicted_prices)
print(&quot;R^2:&quot;, r2)
</code></pre></div>
<h4 id="gradient-descent">Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permanent link">&para;</a></h4>
<p><strong>Gradient descent</strong>Â is an iterative optimization algorithm for minimizing a function, usually a loss function, quantifying the disparity between predicted and actual results. The goal of gradient descent is to find the parameters that minimize the value of the loss function.</p>
<p>Gradient descent derives its name from its working mechanism: takingÂ <em>descents</em>Â along theÂ <em>gradient</em>. It operates in several iterative steps as follows:</p>
<ol>
<li>Choose random values for initial parameters.</li>
<li>Calculate the cost (the difference between actual and predicted value).</li>
<li>Compute the gradient (the steepest slope of the function around that point).</li>
<li>Update the parameters using the gradient.</li>
<li>Repeat steps 2 to 4 until we reach an acceptable error rate or exhaust the maximum iterations.</li>
</ol>
<p>A vital component of gradient descent is the learning rate, which determines the size of the descent towards the optimum solution.</p>
<p>The first step is to calculate the cost function, which takes the form of <span class="arithmatex">\(<span class="arithmatex">\(J(X, y, \theta) = \frac{1}{m}\sum_{i=1}^m(X\cdot \theta - y_i)^2\)</span>\)</span> where J is the cost, X is the data, y is the actual values and <span class="arithmatex">\(\theta\)</span> is the parameters, <span class="arithmatex">\(m\)</span> is the length of <span class="arithmatex">\(y\)</span>. It is calculating the mean square error. </p>
<div class="highlight"><pre><span></span><code>import numpy as np

def cost(X, y, theta):
    m = len(y)
    predictions = X @ theta
    cost = (1/m) * np.sum((predictions - y) ** 2)
    return cost
</code></pre></div>
<p>The second step is to compute the gradient descent function, which will be updated in the iterative loop:
<span class="arithmatex">\(<span class="arithmatex">\(\theta:=\theta-\alpha\frac{1}{m}X^T\cdot(X\cdot \theta - y)\)</span>\)</span> Here <span class="arithmatex">\(\alpha\)</span> is the learning rate, which determines the size of steps in the descent and <span class="arithmatex">\(X^T\)</span> is the transpose of data, which should have been multiplied by 2 but as we take the derivative of the mean squared error we could also consider it to be included as part of the learning rate <span class="arithmatex">\(\alpha\)</span>. </p>
<div class="highlight"><pre><span></span><code>def gradient_descent(X, y, theta, alpha, threshold=0.01):
Â  Â  m = len(y)
Â  Â  cost_history = []
Â  Â  prev_cost = float(&#39;inf&#39;)
Â  Â  iterations = 0
Â  Â  while True:
Â   Â  Â   prediction = X.dot(theta)
Â  Â  Â  Â  theta = theta - (alpha / m) * X.T.dot(prediction - y)
Â  Â  Â  Â  cost = (1/(2*m)) * np.sum((prediction - y) ** 2)
Â  Â  Â  Â  cost_history.append(cost)
Â  Â  Â  Â  if abs(prev_cost - cost) &lt; threshold:
Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  prev_cost = cost
Â  Â  Â  Â  iterations += 1
Â  Â  return theta, cost_history, iterations
</code></pre></div>
<h3 id="support-vector-machine-svm">Support Vector Machine (SVM)<a class="headerlink" href="#support-vector-machine-svm" title="Permanent link">&para;</a></h3>
<h4 id="additional-resources">Additional Resources<a class="headerlink" href="#additional-resources" title="Permanent link">&para;</a></h4>
<p><a href="https://www.youtube.com/watch?v=efR1C6CvhmE&amp;ab_channel=StatQuestwithJoshStarmer">StatQuest Part1 SVM Main Idea</a>
<a href="https://www.youtube.com/watch?v=Toet3EiSFcM&amp;ab_channel=StatQuestwithJoshStarmer">StatQuest Part2 The Polynomial Kernel</a></p>
<h4 id="main-idea-behind-svm">Main Idea behind SVM<a class="headerlink" href="#main-idea-behind-svm" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>Soft Margin Classifier (Support Vector Classifier)</strong>
    When data are 3-dimensional, the <strong>Support Vector Classifier is a 2-dimensional plane in a 3-dimensional space</strong>. In mathematical world, a plane is a "flat affine 2-dimensional subspace (hyperplane)".</p>
<p>But it only works well on data that are perfectly separated into two groups, when it comes down to data that are within certain range versus out-of-range, it cannot handle that well.
- <strong>Support Vector Machine</strong>
In order to make the mathematics possible, SVM use something called kernel functions to systematically find support vector classifiers in higher dimensions.
- <strong>Kernel Functions</strong>
When d = 1, the polynomial kernel computes the relationships between each pair of observations in 1-dimension, and these relationships are used to find a support vector classifier. In summary, the polynomial kernel systematically increases dimensions by setting d, the degree of the polynomial. 
- <strong>Polynomial Kernel</strong>
    <span class="arithmatex">\((a\times b + r)^d\)</span> is the polynomial kernel format, where d sets the dimension of the kernel. Using <span class="arithmatex">\((a\times b + \frac{1}{2})^2\)</span> as an example: <span class="arithmatex">\(<span class="arithmatex">\((a\times b + \frac{1}{2})^2 = (a\times b + \frac{1}{2})(a\times b + \frac{1}{2}) = ab + a^2b^2+\frac{1}{4} = (a,a^2,\frac{1}{2})\cdot (b,b^2,\frac{1}{2})\)</span>\)</span>
    where <span class="arithmatex">\((a,a^2,\frac{1}{2})\)</span> and <span class="arithmatex">\((b,b^2,\frac{1}{2})\)</span> are the coordinates of the data points x-y-z dimensions. <span class="arithmatex">\(r\)</span> and <span class="arithmatex">\(d\)</span> are determined via cross validation. Once we determines the parameters, then we plug in all the pairs of data points and do the math to get the high-dimensional relationships.
- <strong>Radial Function Kernel</strong> 
    Radial function kernel finds support vector classifiers in infinite dimensions but in one / two dimensional data, it behaves like weighted nearest neighborhood model.
    The equation looks like this <span class="arithmatex">\(e^{-\gamma(a-b)^2}\)</span> where <span class="arithmatex">\(a\)</span> and <span class="arithmatex">\(b\)</span> are the x-axis coordinates of two different data points. <span class="arithmatex">\(\gamma\)</span> is the parameter that determines how much influence the pair of data points have on each other.</p>
<h3 id="logistic-regression">Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permanent link">&para;</a></h3>
<p>Logistic regression are the most widely used and most fundamental model that one could use in the ML industry. One should always understand the deduction of logistic regression and application of it, as it is used in medical diagnosis, credit evaluation, email junk categorization, etc. </p>
</li>
</ul>
<h4 id="formulation-behind-logistic-regression">Formulation behind Logistic Regression<a class="headerlink" href="#formulation-behind-logistic-regression" title="Permanent link">&para;</a></h4>
<p>Logistic Regression calculates a raw model output, then transforms it using the sigmoid function, mapping it to a range between 0 and 1, thus making it a probability. The sigmoid function can be defined as <span class="arithmatex">\(S(x) = \frac{1}{1+e^{-x}}\)</span>. This can thus be implemented as:</p>
<div class="highlight"><pre><span></span><code>def sigmoid(z):
    return 1 / (1+np.exp(-z))
</code></pre></div>
<p>The mathematical form of logistic regression can be expressed as follows:
<span class="arithmatex">\(<span class="arithmatex">\(P(Y=1|x) = \frac{1}{1+e^{-(\beta_0+\beta_1x)}}\)</span>\)</span>
where <span class="arithmatex">\(P(Y=1|x)\)</span> is the probability of event <span class="arithmatex">\(Y=1\)</span> given <span class="arithmatex">\(x\)</span>, <span class="arithmatex">\(\beta_0\)</span> and <span class="arithmatex">\(\beta_1\)</span> are parameters of the model, <span class="arithmatex">\(x\)</span> is the input variable and <span class="arithmatex">\(\beta_0+\beta_1x\)</span> is the linear combination of parameters and features. </p>
<p><em>Log-Likelihood</em>Â in Logistic Regression plays a similar role to theÂ <em>Least Squares method</em>Â in Linear Regression. A maximum likelihood estimation method estimates parameters that maximize the likelihood of making the observations we collected. In Logistic Regression, we seek to maximize the log-likelihood.</p>
<p>The cost function for a single training instance in logistic regression can be expressed as <span class="arithmatex">\(-[y\log{(\hat p)+(1-y)\log{(1-\hat p)}}]\)</span> where <span class="arithmatex">\(\hat p\)</span> denotes the predicted probability.</p>
<div class="highlight"><pre><span></span><code>def cost_function(h, y): # h = sigmoid(z) where z = X @ theta
    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()

def logistic_regression(X, y, num_iterations, learning_rate): 
    # Add intercept to X 
    intercept = np.ones((X.shape[0], 1)) 
    X = np.concatenate((intercept, X), axis=1) 

    # Weights initialization 
    theta = np.zeros(X.shape[1]) 
    for i in range(num_iterations): 
        z = np.dot(X, theta) 
        h = sigmoid(z) 
        gradient = np.dot(X.T, (h - y)) / y.size 
        theta -= learning_rate * gradient 

        z = np.dot(X, theta) 
        h = sigmoid(z) 
        loss = cost_function(h, y) 

        if i % 10000 == 0:
            print(f&#39;Loss: {loss}\t&#39;) 

    return theta

def predict_prob(X, theta):
    # Add intercept to X
    intercept = np.ones((X.shape[0], 1))
    X = np.concatenate((intercept, X), axis=1)
    return sigmoid(np.dot(X, theta))

def predict(X, theta, threshold=0.5):
    return predict_prob(X, theta) &gt;= threshold
</code></pre></div>
<h4 id="what-is-the-difference-between-logistic-regression-and-linear-regression">What is the difference between logistic regression and linear regression?<a class="headerlink" href="#what-is-the-difference-between-logistic-regression-and-linear-regression" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>logistic regression is used for categorization whereas linear regression is used for regression problems</strong>. This is the most significant difference between the two. In logistic regression, when given x and hyperparameter <span class="arithmatex">\(\theta\)</span>, we could get the expectation value of the <span class="arithmatex">\(y\)</span> values to predict the categorization of the values. On the other hand, in linear regression, one is solving <span class="arithmatex">\(y' = \theta^Tx\)</span> , which is the approximate of the real relationship of <span class="arithmatex">\(y = \theta^Tx+\epsilon\)</span> where <span class="arithmatex">\(\epsilon\)</span> corresponds to the system error.</li>
<li>The actual logistic regression equation can be formulated via <span class="arithmatex">\(\log{\frac{p}{1-p}}=\theta^Tx\)</span>, where <span class="arithmatex">\(p=P(y=1|x)\)</span> , corresponding to given x the probability of y being positive. <strong>Thus the most important difference between logistic regression and linear regression would be that the logistic regression <span class="arithmatex">\(y\)</span>s are discretized whereas the linear regression <span class="arithmatex">\(y\)</span>s are continuous.</strong> When <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(\theta\)</span> are given, logistic regression can also be seen as generalized linear models where <span class="arithmatex">\(y\)</span> follows the binary distribution, whereas  when using least-squares for linear regression we view <span class="arithmatex">\(y\)</span> follows the normal distribution. </li>
</ul>
<h4 id="what-is-the-same-between-logistic-regression-and-linear-regression">What is the same between logistic regression and linear regression?<a class="headerlink" href="#what-is-the-same-between-logistic-regression-and-linear-regression" title="Permanent link">&para;</a></h4>
<ul>
<li>They both used maximum likelihood estimation for modeling the training data. </li>
<li>They both could use gradient descent for getting the hyperparameters, and it is also a common strategy that all the supervised learning methods use.</li>
</ul>
<h4 id="the-general-logic-behind-regression">The general logic behind regression<a class="headerlink" href="#the-general-logic-behind-regression" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>Inputs: X (NÃ—d), y (N,), model âˆˆ {&quot;linear&quot;,&quot;logistic&quot;}
Hyperparams: learning_rate (lr), lambda (L2), max_iters, tol, patience
Prep:
  Xb = concat([ones(N,1), X])        # add bias column
  w = zeros(d+1)                     # includes bias at index 0
  mask = [0, 1, 1, ..., 1]           # no L2 on bias

For t in 1..max_iters:
  z = Xb @ w
  if model == &quot;linear&quot;:
      pred = z
      loss_data = (1/(2N)) * sum((pred - y)^2)
  else:  # logistic
      pred = sigmoid(z)              # clip to [eps, 1-eps] for stability
      loss_data = -(1/N) * sum(y*log(pred) + (1-y)*log(1-pred))

  loss = loss_data + lambda * sum((w*mask)^2)
  grad = (1/N) * (Xb.T @ (pred - y)) + 2*lambda*(w*mask)
  w = w - learning_rate * grad
  if norm(grad) &lt; tol or early_stopping_on_val(loss): break

Return w
</code></pre></div>
<h4 id="note-for-binomial-distribution-vs-normal-distribution">Note for binomial distribution vs normal distribution<a class="headerlink" href="#note-for-binomial-distribution-vs-normal-distribution" title="Permanent link">&para;</a></h4>
<p>The main difference between a binomial distribution and a normal distribution lies in the type of data they describe:Â ==binomial distributions deal with discrete data from a fixed number of trials, while normal distributions describe continuous data that tends to cluster around a mean==.Â Binomial distributions are characterized by a fixed number of trials, each with two possible outcomes (success or failure), while normal distributions are continuous, symmetric, and have a bell-shaped curve.</p>
<h3 id="decision-tree">Decision Tree<a class="headerlink" href="#decision-tree" title="Permanent link">&para;</a></h3>
<p>Decision trees are often used in marketing or biomedical industries as the tree-based structure is similar to sales or diagnosis use cases. Hence, when using decision tree as key component of the ensemble method, one could get random forest or gradient boosted decision tree models, etc. Fully grown decision tree model has its characters of being direct and easy-to-explain, hence it would be also important as the ensemble method section prerequisites. Overall, the formulation of decision tree involves 1) feature selection, 2) tree construction and 3) tree pruning. </p>
<h4 id="structuring-a-decision-tree">Structuring a decision tree<a class="headerlink" href="#structuring-a-decision-tree" title="Permanent link">&para;</a></h4>
<p>A decision tree starts at a node, called root, which breaks down into branches. Each branch then further splits into more branches, building a hierarchical network. The final branches with no more splits are referred to as leaf nodes.</p>
<h4 id="understanding-gini-index">Understanding Gini Index<a class="headerlink" href="#understanding-gini-index" title="Permanent link">&para;</a></h4>
<p><em><strong>Note</strong></em>: A more clear explanation can be found in videos:
- <a href="https://www.youtube.com/watch?v=_L39rN6gz7Y&amp;t=18s">StatQuest: Decision and Classification Trees, Clearly Explained!!!</a>
- <a href="https://www.youtube.com/watch?v=wpNl-JwwplA">StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data</a>
- <a href="https://www.youtube.com/watch?v=D0efHEJsfHo">StatQuest: How to Prune Regression Trees, Clearly Explained!!!</a>
A Gini Index endeavors to quantify the disorder within these groups. A greater Gini Index score signifies more disorder. The formula of Gini Index can be represented as <span class="arithmatex">\(G = 1-\sum_{i=1}^n p_i^2\)</span> where <span class="arithmatex">\(G\)</span> is the Gini index or coefficient, <span class="arithmatex">\(p_i\)</span> is the proportion of individuals in the <span class="arithmatex">\(i\)</span>th group, and the sum is taken over <span class="arithmatex">\(n\)</span> groups. Gini index is used to describe the data purity, which has similar concept with information entropy. 
<span class="arithmatex">\(<span class="arithmatex">\(\text{Gini}(D) = 1 - \sum_{k=1}^n(\frac{C_k}{D})^2\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(\text{Gini}(D|A) = \sum_{i=1}^n\frac{|D_i|}{|D|}\text{Gini}(D_i)\)</span>\)</span>
Now let's use an example to better understand how to compute Gini index:</p>
<table>
<thead>
<tr>
<th></th>
<th>Loves Popcorn</th>
<th>Loves Soda</th>
<th>Age</th>
<th>like movie</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>Y</td>
<td>Y</td>
<td>7</td>
<td>N</td>
</tr>
<tr>
<td>B</td>
<td>Y</td>
<td>N</td>
<td>12</td>
<td>N</td>
</tr>
<tr>
<td>C</td>
<td>N</td>
<td>Y</td>
<td>18</td>
<td>Y</td>
</tr>
<tr>
<td>D</td>
<td>N</td>
<td>Y</td>
<td>35</td>
<td>Y</td>
</tr>
<tr>
<td>E</td>
<td>Y</td>
<td>Y</td>
<td>38</td>
<td>Y</td>
</tr>
<tr>
<td>F</td>
<td>Y</td>
<td>N</td>
<td>50</td>
<td>N</td>
</tr>
<tr>
<td>G</td>
<td>N</td>
<td>N</td>
<td>83</td>
<td>N</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Loves Popcorn</th>
<th style="text-align: center;">Loves Soda</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><img src="resources/gini_index_1.png" alt="drawing" style="width:200px;"/></td>
<td style="text-align: center;"><img src="resources/gini_index_2.png" alt="drawing" style="width:200px;"/></td>
</tr>
</tbody>
</table>
<p>All the three leaves except for the fourth one are called impure leaves, where the fourth one is called a pure leaf node. As both leaf nodes from <code>loves Popcorn</code> are impure but there is only one node from <code>Loves Soda</code> being impure, it means that the <code>Loves Soda</code> does a better job predicting who will and will not the movie. </p>
<div class="arithmatex">\[\text{Gini Impurity for a leaf} = 1 - (\text{the probability of "Yes"}) ^ 2 - (\text{the probability of "No"}) ^ 2\]</div>
<div class="arithmatex">\[\text{Gini Impurity (Loves Movie | Loves Popcorn)} = 1 - (\frac{1}{1+3})^2 - (\frac{3}{1+3})^2 = 0.375\]</div>
<div class="arithmatex">\[\text{Gini Impurity (Loves Movie | Hates Popcorn)} = 1 - (\frac{2}{1+2})^2 - (\frac{1}{1+2})^2 = 0.444\]</div>
<div class="arithmatex">\[\text{Total Gini Impurity} = \text{weighted avg of Gini for the leaves} = (\frac{1+3}{1+3+2+1})\cdot(0.375)+\frac{3}{4+3}(0.444)\]</div>
<h4 id="implementation-of-decision-tree-splits">Implementation of decision tree splits<a class="headerlink" href="#implementation-of-decision-tree-splits" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>groups = [
    \[\[&#39;Red&#39;], [&#39;Blue&#39;], [&#39;Red&#39;\]\],
    \[\[&#39;Blue&#39;], [&#39;Red&#39;], [&#39;Blue&#39;], [&#39;Blue&#39;\]\],
]
classes = [&#39;Red&#39;, &#39;Blue&#39;]

n_instances = float(sum([len(group) for group in groups]))

def gini_index(groups, classes):
    n_instances = float(sum([len(group) for group in groups]))
    gini = 0.0
    for group in groups:
        size = len(group)
        if size == 0:
            continue
        score = 0.0
        for class_val in classes:
            p = [row[-1] for row in group].count(class_val) / size
            score += p * p gini # summed probabilities, 1 - score = gini impurity
        gini += (1.0 - score) * (size / n_instances)
    return gini

def test_split(index, value, dataset):
    left, right = list(), list()
    for row in dataset:
        if row[index] &lt; value:
            left.append(row)
        else:
            right.append(row)
    return left, right
</code></pre></div>
<h4 id="information-gain">Information Gain<a class="headerlink" href="#information-gain" title="Permanent link">&para;</a></h4>
<h5 id="max-information-gain">Max Information Gain<a class="headerlink" href="#max-information-gain" title="Permanent link">&para;</a></h5>
<p>For a sample set D, there are K categories, the empirical entropy for this set D can be expressed as <span class="arithmatex">\(H(D) = -\sum_{k=1}^K \frac{|C_k|}{D}\log_2\frac{C_k}{D}\)</span>.</p>
<h2 id="unsupervised-learnings">Unsupervised Learnings<a class="headerlink" href="#unsupervised-learnings" title="Permanent link">&para;</a></h2>
<p>We may encounter problems such that providing the machine a tons of feature data and looking for the machine to learn the pattern or structure from the data, for example the video platforms would like to categorize the users from their activities for different recommendation strategies, or looking for relationship between whether the video playing smooth or not vs their relationship with user unsubscribe. These problems are called "unsupervised learnings", which does not like the supervised learnings where we expect to see outputs or predictions. The unsupervised learning inputs does not contain label information, instead it needs to dig into the internal data relationship from the algorithm model. <strong>There are two main categories of the unsupervised learnings: data clustering or feature variable correlation (using correlation analysis for relationships between variables)</strong>. </p>
<h3 id="k-nearest-neighbors-k-nn-algorithm">K-Nearest Neighbors (k-NN) Algorithm<a class="headerlink" href="#k-nearest-neighbors-k-nn-algorithm" title="Permanent link">&para;</a></h3>
<p>The kNN algorithm works on a basic principle: a data point is likely to be in the same category as the data points it is closest to. Note that choosing 'k' significantly impacts our model. A low 'k' might capture more noise in the data, whereas a high 'k' is computationally expensive.</p>
<h3 id="euclidean-distance-calculation">Euclidean Distance Calculation<a class="headerlink" href="#euclidean-distance-calculation" title="Permanent link">&para;</a></h3>
<p>In k-NN, classification is determined by weighing the distance between data points. Euclidean distance is a frequently used metric that calculates the shortest straight-line distance <span class="arithmatex">\(\sqrt{(x_1-x_2)^2 + (y_1 - y_2)^2}\)</span> between two data points <span class="arithmatex">\((x_1, y_1)\)</span> and <span class="arithmatex">\((x_2, y_2)\)</span> in a Euclidean space. </p>
<div class="highlight"><pre><span></span><code>import math

# The &#39;euclidean_distance&#39; function computes the Euclidean distance between two points
def euclidean_distance(point1, point2):
    squares = [(p - q) ** 2 for p, q in zip(point1, point2)] # Calculate squared distance for each dimension
    return math.sqrt(sum(squares)) # Return the square root of the sum of squares

# Test it
point1 = (1, 2) # The coordinates of the first point
point2 = (4, 6) # The coordinates of the second point
print(euclidean_distance(point1, point2)) # 5.0
</code></pre></div>
<h3 id="actual-knn-algorithm">Actual KNN Algorithm<a class="headerlink" href="#actual-knn-algorithm" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>from collections import Counter
import numpy as np

def k_nearest_neighbors(data, query, k, distance_fn):
Â  Â  neighbor_distances_and_indices = []
Â  Â  # Compute distance from each training data point
Â  Â  for idx, label in enumerate(data):
Â  Â  Â  Â  distance = euclidean_distance(label[0], query)
Â  Â  neighbor_distances_and_indices.append((distance, idx))
Â  Â  # Sort array by distance
Â  Â  sorted_neighbor_distances_and_indices = sorted(neighbor_distances_and_indices)
Â  Â  # Select k closest data points
Â  Â  k_nearest_distances_and_indices = sorted_neighbor_distances_and_indices[:k]
Â  Â  # Obtain class labels for those k data points
Â  Â  k_nearest_labels = [data[i][1] for distance, i in k_nearest_distances_and_indices]
Â  Â  # Majority vote
Â  Â  most_common = Counter(k_nearest_labels).most_common(1)
Â  Â  return most_common[0][0] # Return the label of the class that receives the majority vote

def euclidean_distance(point1, point2):
Â  Â  distance = sum((p - q) ** 2 for p, q in zip(point1, point2))
Â  Â  return np.sqrt(distance)
Â  Â  
def mannhattan_distance(point1, point2):
Â  Â  return np.sum(np.abs(p - q) for p, q in zip(point1, point2))

data = [
Â  Â  ((2, 3), 0),
Â  Â  ((5, 4), 0),
Â  Â  ((9, 6), 1),
Â  Â  ((4, 7), 0),
Â  Â  ((8, 1), 1),
Â  Â  ((7, 2), 1)
]
query = (7,6)
k=2

class_label = k_nearest_neighbors(data, query, k, distance_fn)
print(class_label)
</code></pre></div>
<h3 id="k-means-clustering">K-means Clustering<a class="headerlink" href="#k-means-clustering" title="Permanent link">&para;</a></h3>
<p>Algorithms such as SVM, logistic regression, decision trees are more for the categorization, i.e. based on the known labelled samples, classifiers are training so that it could apply the same logic on unlabeled samples. Unlike the classification problems, clustering is directly categorize the samples without any previously known labelling. </p>
<p>Classification belongs to supervised learning whereas clustering is a type of unsupervised learning algorithm. K-means clustering, as one type of the most basic and fundamental clustering algorithm, has the main idea of iteratively finding the way of cutting the space into K clusters, so that the loss function is the lowest. The loss function can be defined as the sum of squared error distance of each sample from their clustered centers:
<span class="arithmatex">\(<span class="arithmatex">\(J(c,\mu) = \sum_{i=1}^M ||x_i - \mu_{c_i}||^2\)</span>\)</span> where <span class="arithmatex">\(x_i\)</span> represents the samples, <span class="arithmatex">\(c_i\)</span> represents the cluster that <span class="arithmatex">\(x_i\)</span> belongs to, <span class="arithmatex">\(mu_{c_i}\)</span> corresponds to the center of the cluster that <span class="arithmatex">\(x_i\)</span>'s located in and <span class="arithmatex">\(M\)</span> is the total number of samples.</p>
<h4 id="k-means-clustering-algorithm-in-steps">K-means clustering algorithm in steps<a class="headerlink" href="#k-means-clustering-algorithm-in-steps" title="Permanent link">&para;</a></h4>
<p>The goal of K-means clustering is to categorize the dataset of interest into K-clusters, and also provides the cluster center corresponding to each data points:
1. data engineering and cleaning: normalization and outlier removal.
2. randomly pick K-cluster centers, labelled as <span class="arithmatex">\(\mu_1^{(0)}, \mu_2^{(0)}, ..., \mu_K^{(0)}\)</span> 
3. define the loss function to be <span class="arithmatex">\(J(c,\mu) = \min_{\mu} \min_{c} \sum_{i=1}^M ||x_i - \mu_{c_i}||^2\)</span> 
4. iterate through the process below by t times, where t denotes the number of iterations:
    1. for every sample <span class="arithmatex">\(x_i\)</span>, categorize it to the cluster that has shortest distance <span class="arithmatex">\(<span class="arithmatex">\(c_i^{(t)} \leftarrow {\arg\min}_k ||x_i - \mu_k^{(t)}||^2\)</span>\)</span>
    2. for every cluster k, recalculate the center: <span class="arithmatex">\(<span class="arithmatex">\(\mu_k^{(t+1)}\leftarrow {\arg\min}_\mu \sum_{i:c_i^{(t)}=k} ||x_i - \mu||^2\)</span>\)</span>
<div class="highlight"><pre><span></span><code># k-Means algorithm
def k_means(data, centers, k):
    while True:
        clusters = [[] for _ in range(k)] 

        # Assign data points to the closest center
        for point in data:
            distances = [distance(point, center) for center in centers]
            index = distances.index(min(distances)) 
            clusters[index].append(point)

        # Update centers to be the mean of points in a cluster
        new_centers = []
        for cluster in clusters:
            center = (sum([point[0] for point in cluster])/len(cluster), 
                      sum([point[1] for point in cluster])/len(cluster)) 
            new_centers.append(center)

        # Break loop if centers don&#39;t change significantly
        if max([distance(new, old) for new, old in zip(new_centers, centers)]) &lt; 0.0001:
            break
        else:
            centers = new_centers
    return clusters, centers
</code></pre></div></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>need to work on the definition of this and learn more about information theory&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>Please read through the recommendation system based on matrix vectorization to get a better idea on how recommenders are built based on SVD and matrices&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>







  
  






                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.sections", "navigation.top", "content.code.copy", "content.code.annotate", "content.action.edit", "content.tabs.link", "toc.integrate", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="../../javascripts/floating-nav.js"></script>
      
        <script src="https://unpkg.com/mermaid@10/dist/mermaid.min.js"></script>
      
    
  </body>
</html>